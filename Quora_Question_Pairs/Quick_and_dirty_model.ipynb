{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick and dirty model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to provide a very simple model that will serve as a baseline for future modeling. The model used is a simple densely connected neural network with shared word embeddings for both input questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MAX_WORDS = 300\n",
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tokenizer(df, max_words):\n",
    "    all_texts = pd.concat([df['question1'], df['question2']])\n",
    "    tokenizer = Tokenizer(max_words)\n",
    "    tokenizer.fit_on_texts(str(text) for text in all_texts.to_list())\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_question_sequences(df, max_len):\n",
    "    return pad_sequences(df['q1_sequences'], maxlen=max_len), pad_sequences(df['q2_sequences'], maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = pd.concat((train['question1'], train['question2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(MAX_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(str(text) for text in all_texts.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['q1_sequences'] = tokenizer.texts_to_sequences(str(text) for text in train['question1'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['q2_sequences'] = tokenizer.texts_to_sequences(str(text) for text in train['question2'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train.drop('is_duplicate', axis=1),\n",
    "                                                    train['is_duplicate'],\n",
    "                                                    test_size=0.05,\n",
    "                                                    stratify=train['is_duplicate'].values,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train, q2_train, q1_test, q2_test = pad_sequences(x_train['q1_sequences'], maxlen=MAX_LEN), pad_sequences(x_train['q2_sequences'], maxlen=MAX_LEN), pad_sequences(x_test['q1_sequences'], maxlen=MAX_LEN), pad_sequences(x_test['q2_sequences'], maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    input_q1 = layers.Input(shape=(MAX_LEN,))\n",
    "    input_q2 = layers.Input(shape=(MAX_LEN,))\n",
    "    \n",
    "    shared_embeddings = layers.Embedding(MAX_WORDS, 300, input_length=MAX_LEN)\n",
    "    \n",
    "    q1_embeddings = shared_embeddings(input_q1)\n",
    "    q2_embeddings = shared_embeddings(input_q2)\n",
    "    \n",
    "    #sub = layers.Subtract()([q1_embeddings, q2_embeddings])\n",
    "    concat = layers.Concatenate()([q1_embeddings, q2_embeddings])\n",
    "    flattened = layers.Flatten()(concat)\n",
    "    \n",
    "    dense_1 = layers.Dense(256, activation='relu')(flattened)\n",
    "    dense_2 = layers.Dense(128, activation='relu')(dense_1)\n",
    "    dense_3 = layers.Dense(128, activation='relu')(dense_2)\n",
    "    dense_4 = layers.Dense(64, activation='relu')(dense_3)\n",
    "    \n",
    "    out = layers.Dense(1, activation='sigmoid')(dense_4)\n",
    "    \n",
    "    return Model(inputs=[input_q1, input_q2], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMaker:\n",
    "    \n",
    "    def __init__(self, max_words, dimensions, input_length):\n",
    "        self._max_words = max_words\n",
    "        self._dimensions = dimensions\n",
    "        self._input_length = input_length\n",
    "    \n",
    "    def _prepare_embeddings_model(self):\n",
    "        input_q1 = layers.Input(shape=(self._input_length,))\n",
    "        input_q2 = layers.Input(shape=(self._input_length,))\n",
    "        \n",
    "        shared_embeddings = layers.Embedding(self._max_words, self._dimensions, input_length=self._input_length)\n",
    "        q1_embeddings = shared_embeddings(input_q1)\n",
    "        q2_embeddings = shared_embeddings(input_q2)\n",
    "        \n",
    "        concat = layers.Concatenate()([q1_embeddings, q2_embeddings])\n",
    "        flattened = layers.Flatten()(concat)\n",
    "        \n",
    "        return Model(inputs=[input_q1, input_q2], outputs=flattened)\n",
    "    \n",
    "    def prepare_model(self, n_neurons):\n",
    "        input_q1 = layers.Input(shape=(self._input_length,))\n",
    "        input_q2 = layers.Input(shape=(self._input_length,))\n",
    "        \n",
    "        embeddings_model = self._prepare_embeddings_model()\n",
    "        \n",
    "        int_result = embeddings_model(inputs=[input_q1, input_q2])\n",
    "                \n",
    "        model = Sequential()\n",
    "        \n",
    "        for n in n_neurons:\n",
    "            model.add(layers.Dense(n, activation='relu'))\n",
    "            \n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        out = model(int_result)\n",
    "        \n",
    "        return Model(inputs=[input_q1, input_q2], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 364871 samples, validate on 19204 samples\n",
      "Epoch 1/10\n",
      "364871/364871 [==============================] - 37s 100us/step - loss: 0.5448 - acc: 0.7207 - val_loss: 0.5140 - val_acc: 0.7428\n",
      "Epoch 2/10\n",
      "364871/364871 [==============================] - 36s 99us/step - loss: 0.4934 - acc: 0.7522 - val_loss: 0.5014 - val_acc: 0.7536\n",
      "Epoch 3/10\n",
      "364871/364871 [==============================] - 36s 100us/step - loss: 0.4561 - acc: 0.7733 - val_loss: 0.4886 - val_acc: 0.7613\n",
      "Epoch 4/10\n",
      "364871/364871 [==============================] - 37s 100us/step - loss: 0.4156 - acc: 0.7960 - val_loss: 0.4944 - val_acc: 0.7625\n",
      "Epoch 5/10\n",
      "364871/364871 [==============================] - 36s 100us/step - loss: 0.3747 - acc: 0.8185 - val_loss: 0.5266 - val_acc: 0.7464\n",
      "Epoch 6/10\n",
      "364871/364871 [==============================] - 36s 99us/step - loss: 0.3373 - acc: 0.8387 - val_loss: 0.5605 - val_acc: 0.7678\n",
      "Epoch 7/10\n",
      "364871/364871 [==============================] - 36s 97us/step - loss: 0.3049 - acc: 0.8555 - val_loss: 0.5760 - val_acc: 0.7610\n",
      "Epoch 8/10\n",
      "364871/364871 [==============================] - 35s 97us/step - loss: 0.2774 - acc: 0.8699 - val_loss: 0.6246 - val_acc: 0.7644\n",
      "Epoch 9/10\n",
      "364871/364871 [==============================] - 36s 97us/step - loss: 0.2539 - acc: 0.8817 - val_loss: 0.6485 - val_acc: 0.7654\n",
      "Epoch 10/10\n",
      "364871/364871 [==============================] - 36s 98us/step - loss: 0.2340 - acc: 0.8923 - val_loss: 0.7345 - val_acc: 0.7597\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([q1_train, q2_train],\n",
    "                    y_train.values,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_maker = ModelMaker(max_words=MAX_WORDS, dimensions=300, input_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_maker.prepare_model([64, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 364871 samples, validate on 19204 samples\n",
      "Epoch 1/10\n",
      "364871/364871 [==============================] - 18s 50us/step - loss: 0.5498 - acc: 0.7177 - val_loss: 0.5227 - val_acc: 0.7376\n",
      "Epoch 2/10\n",
      "364871/364871 [==============================] - 18s 49us/step - loss: 0.5036 - acc: 0.7448 - val_loss: 0.5058 - val_acc: 0.7438\n",
      "Epoch 3/10\n",
      "364871/364871 [==============================] - 18s 49us/step - loss: 0.4771 - acc: 0.7612 - val_loss: 0.5005 - val_acc: 0.7546\n",
      "Epoch 4/10\n",
      "364871/364871 [==============================] - 18s 49us/step - loss: 0.4522 - acc: 0.7751 - val_loss: 0.4936 - val_acc: 0.7566\n",
      "Epoch 5/10\n",
      "364871/364871 [==============================] - 18s 48us/step - loss: 0.4277 - acc: 0.7898 - val_loss: 0.4989 - val_acc: 0.7581\n",
      "Epoch 6/10\n",
      "364871/364871 [==============================] - 18s 49us/step - loss: 0.4047 - acc: 0.8031 - val_loss: 0.5096 - val_acc: 0.7567\n",
      "Epoch 7/10\n",
      "364871/364871 [==============================] - 18s 49us/step - loss: 0.3836 - acc: 0.8142 - val_loss: 0.5311 - val_acc: 0.7584\n",
      "Epoch 8/10\n",
      "364871/364871 [==============================] - 18s 49us/step - loss: 0.3641 - acc: 0.8252 - val_loss: 0.5385 - val_acc: 0.7593\n",
      "Epoch 9/10\n",
      "364871/364871 [==============================] - 18s 49us/step - loss: 0.3476 - acc: 0.8333 - val_loss: 0.5603 - val_acc: 0.7543\n",
      "Epoch 10/10\n",
      "364871/364871 [==============================] - 18s 49us/step - loss: 0.3320 - acc: 0.8428 - val_loss: 0.5764 - val_acc: 0.7559\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([q1_train, q2_train],\n",
    "                    y_train.values,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [\n",
    "    [64, 32],\n",
    "    [64, 64, 32],\n",
    "    [128, 128, 64, 32],\n",
    "    [128, 128, 64, 64, 32],\n",
    "    [256, 256, 128, 128, 64, 64, 32, 32, 16, 16, 8]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 364871 samples, validate on 19204 samples\n",
      "Epoch 1/10\n",
      "364871/364871 [==============================] - 17s 47us/step - loss: 0.5513 - acc: 0.7167 - val_loss: 0.5278 - val_acc: 0.7340\n",
      "Epoch 2/10\n",
      "364871/364871 [==============================] - 17s 46us/step - loss: 0.5053 - acc: 0.7432 - val_loss: 0.5066 - val_acc: 0.7451\n",
      "Epoch 3/10\n",
      "364871/364871 [==============================] - 17s 46us/step - loss: 0.4771 - acc: 0.7602 - val_loss: 0.5001 - val_acc: 0.7517\n",
      "Epoch 4/10\n",
      "364871/364871 [==============================] - 17s 46us/step - loss: 0.4504 - acc: 0.7763 - val_loss: 0.4979 - val_acc: 0.7582\n",
      "Epoch 5/10\n",
      "364871/364871 [==============================] - 17s 46us/step - loss: 0.4246 - acc: 0.7907 - val_loss: 0.5040 - val_acc: 0.7591\n",
      "Epoch 6/10\n",
      "364871/364871 [==============================] - 17s 46us/step - loss: 0.4010 - acc: 0.8039 - val_loss: 0.5102 - val_acc: 0.7621\n",
      "Epoch 7/10\n",
      "364871/364871 [==============================] - 17s 46us/step - loss: 0.3791 - acc: 0.8164 - val_loss: 0.5246 - val_acc: 0.7617\n",
      "Epoch 8/10\n",
      "364871/364871 [==============================] - 17s 46us/step - loss: 0.3595 - acc: 0.8274 - val_loss: 0.5436 - val_acc: 0.7595\n",
      "Epoch 9/10\n",
      "364871/364871 [==============================] - 17s 46us/step - loss: 0.3422 - acc: 0.8362 - val_loss: 0.5617 - val_acc: 0.7569\n",
      "Epoch 10/10\n",
      "364871/364871 [==============================] - 17s 46us/step - loss: 0.3269 - acc: 0.8444 - val_loss: 0.5845 - val_acc: 0.7593\n",
      "Train on 364871 samples, validate on 19204 samples\n",
      "Epoch 1/10\n",
      "364871/364871 [==============================] - 18s 51us/step - loss: 0.5528 - acc: 0.7157 - val_loss: 0.5295 - val_acc: 0.7338\n",
      "Epoch 2/10\n",
      "364871/364871 [==============================] - 18s 49us/step - loss: 0.5066 - acc: 0.7443 - val_loss: 0.5094 - val_acc: 0.7460\n",
      "Epoch 3/10\n",
      "364871/364871 [==============================] - 18s 49us/step - loss: 0.4766 - acc: 0.7611 - val_loss: 0.4994 - val_acc: 0.7516\n",
      "Epoch 4/10\n",
      "364871/364871 [==============================] - 18s 49us/step - loss: 0.4498 - acc: 0.7767 - val_loss: 0.4936 - val_acc: 0.7535\n",
      "Epoch 5/10\n",
      "364871/364871 [==============================] - 18s 49us/step - loss: 0.4246 - acc: 0.7910 - val_loss: 0.5035 - val_acc: 0.7548\n",
      "Epoch 6/10\n",
      "364871/364871 [==============================] - 18s 49us/step - loss: 0.4002 - acc: 0.8051 - val_loss: 0.5156 - val_acc: 0.7585\n",
      "Epoch 7/10\n",
      "364871/364871 [==============================] - 18s 50us/step - loss: 0.3786 - acc: 0.8174 - val_loss: 0.5196 - val_acc: 0.7559\n",
      "Epoch 8/10\n",
      "364871/364871 [==============================] - 18s 49us/step - loss: 0.3591 - acc: 0.8274 - val_loss: 0.5386 - val_acc: 0.7569\n",
      "Epoch 9/10\n",
      "364871/364871 [==============================] - 18s 50us/step - loss: 0.3419 - acc: 0.8367 - val_loss: 0.5496 - val_acc: 0.7602\n",
      "Epoch 10/10\n",
      "364871/364871 [==============================] - 18s 50us/step - loss: 0.3270 - acc: 0.8444 - val_loss: 0.5752 - val_acc: 0.7561\n",
      "Train on 364871 samples, validate on 19204 samples\n",
      "Epoch 1/10\n",
      "364871/364871 [==============================] - 26s 70us/step - loss: 0.5481 - acc: 0.7184 - val_loss: 0.5213 - val_acc: 0.7381\n",
      "Epoch 2/10\n",
      "364871/364871 [==============================] - 25s 68us/step - loss: 0.4993 - acc: 0.7485 - val_loss: 0.5021 - val_acc: 0.7484\n",
      "Epoch 3/10\n",
      "364871/364871 [==============================] - 25s 68us/step - loss: 0.4672 - acc: 0.7667 - val_loss: 0.4949 - val_acc: 0.7545\n",
      "Epoch 4/10\n",
      "364871/364871 [==============================] - 25s 69us/step - loss: 0.4342 - acc: 0.7857 - val_loss: 0.4972 - val_acc: 0.7580\n",
      "Epoch 5/10\n",
      "364871/364871 [==============================] - 25s 68us/step - loss: 0.4018 - acc: 0.8033 - val_loss: 0.5049 - val_acc: 0.7616\n",
      "Epoch 6/10\n",
      "364871/364871 [==============================] - 25s 68us/step - loss: 0.3707 - acc: 0.8207 - val_loss: 0.5206 - val_acc: 0.7616\n",
      "Epoch 7/10\n",
      "364871/364871 [==============================] - 25s 69us/step - loss: 0.3431 - acc: 0.8352 - val_loss: 0.5572 - val_acc: 0.7616\n",
      "Epoch 8/10\n",
      "364871/364871 [==============================] - 25s 69us/step - loss: 0.3188 - acc: 0.8477 - val_loss: 0.5796 - val_acc: 0.7618\n",
      "Epoch 9/10\n",
      "364871/364871 [==============================] - 25s 69us/step - loss: 0.2969 - acc: 0.8595 - val_loss: 0.6132 - val_acc: 0.7588\n",
      "Epoch 10/10\n",
      "364871/364871 [==============================] - 25s 69us/step - loss: 0.2791 - acc: 0.8689 - val_loss: 0.6704 - val_acc: 0.7622\n",
      "Train on 364871 samples, validate on 19204 samples\n",
      "Epoch 1/10\n",
      "364871/364871 [==============================] - 27s 74us/step - loss: 0.5518 - acc: 0.7167 - val_loss: 0.5237 - val_acc: 0.7366\n",
      "Epoch 2/10\n",
      "364871/364871 [==============================] - 26s 72us/step - loss: 0.5008 - acc: 0.7471 - val_loss: 0.4981 - val_acc: 0.7525\n",
      "Epoch 3/10\n",
      "364871/364871 [==============================] - 26s 72us/step - loss: 0.4684 - acc: 0.7653 - val_loss: 0.4976 - val_acc: 0.7560\n",
      "Epoch 4/10\n",
      "364871/364871 [==============================] - 26s 72us/step - loss: 0.4365 - acc: 0.7846 - val_loss: 0.4903 - val_acc: 0.7611\n",
      "Epoch 5/10\n",
      "364871/364871 [==============================] - 26s 71us/step - loss: 0.4049 - acc: 0.8025 - val_loss: 0.5018 - val_acc: 0.7612\n",
      "Epoch 6/10\n",
      "364871/364871 [==============================] - 26s 71us/step - loss: 0.3743 - acc: 0.8188 - val_loss: 0.5184 - val_acc: 0.7644\n",
      "Epoch 7/10\n",
      "364871/364871 [==============================] - 26s 72us/step - loss: 0.3473 - acc: 0.8334 - val_loss: 0.5372 - val_acc: 0.7659\n",
      "Epoch 8/10\n",
      "364871/364871 [==============================] - 27s 73us/step - loss: 0.3229 - acc: 0.8460 - val_loss: 0.5821 - val_acc: 0.7601\n",
      "Epoch 9/10\n",
      "364871/364871 [==============================] - 27s 73us/step - loss: 0.3025 - acc: 0.8562 - val_loss: 0.6166 - val_acc: 0.7624\n",
      "Epoch 10/10\n",
      "364871/364871 [==============================] - 27s 74us/step - loss: 0.2840 - acc: 0.8663 - val_loss: 0.6539 - val_acc: 0.7563\n",
      "Train on 364871 samples, validate on 19204 samples\n",
      "Epoch 1/10\n",
      "364871/364871 [==============================] - 44s 121us/step - loss: 0.6612 - acc: 0.6304 - val_loss: 0.6605 - val_acc: 0.6271\n",
      "Epoch 2/10\n",
      "364871/364871 [==============================] - 43s 119us/step - loss: 0.6584 - acc: 0.6310 - val_loss: 0.6605 - val_acc: 0.6271\n",
      "Epoch 3/10\n",
      "364871/364871 [==============================] - 44s 120us/step - loss: 0.6584 - acc: 0.6310 - val_loss: 0.6605 - val_acc: 0.6271\n",
      "Epoch 4/10\n",
      "364871/364871 [==============================] - 42s 116us/step - loss: 0.6584 - acc: 0.6310 - val_loss: 0.6605 - val_acc: 0.6271\n",
      "Epoch 5/10\n",
      "364871/364871 [==============================] - 42s 116us/step - loss: 0.6584 - acc: 0.6310 - val_loss: 0.6605 - val_acc: 0.6271\n",
      "Epoch 6/10\n",
      "364871/364871 [==============================] - 42s 116us/step - loss: 0.6584 - acc: 0.6310 - val_loss: 0.6605 - val_acc: 0.6271\n",
      "Epoch 7/10\n",
      "364871/364871 [==============================] - 44s 120us/step - loss: 0.6584 - acc: 0.6310 - val_loss: 0.6605 - val_acc: 0.6271\n",
      "Epoch 8/10\n",
      "364871/364871 [==============================] - 44s 121us/step - loss: 0.6584 - acc: 0.6310 - val_loss: 0.6605 - val_acc: 0.6271\n",
      "Epoch 9/10\n",
      "364871/364871 [==============================] - 44s 120us/step - loss: 0.6584 - acc: 0.6310 - val_loss: 0.6605 - val_acc: 0.6271\n",
      "Epoch 10/10\n",
      "364871/364871 [==============================] - 43s 118us/step - loss: 0.6584 - acc: 0.6310 - val_loss: 0.6606 - val_acc: 0.6271\n"
     ]
    }
   ],
   "source": [
    "model_maker = ModelMaker(max_words=MAX_WORDS, dimensions=300, input_length=MAX_LEN)\n",
    "\n",
    "results = []\n",
    "for layers_size in layer_sizes:\n",
    "    model = model_maker.prepare_model(layers_size)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit([q1_train, q2_train],\n",
    "                    y_train.values,\n",
    "                    batch_size=128,\n",
    "                    epochs=10,\n",
    "                    validation_split=0.05)\n",
    "    \n",
    "    results.append(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
