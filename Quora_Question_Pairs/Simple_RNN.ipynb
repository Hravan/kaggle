{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext import data\n",
    "\n",
    "from quoraquestionpairs.data import get_dataset\n",
    "from quoraquestionpairs.neuralnets import RNNGRUSequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = data.Field(tokenize='spacy',\n",
    "                      lower=True,\n",
    "                      pad_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = get_dataset('data/train.csv', question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = data_df.split(0.95, stratified=True, strata_field='is_duplicate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question.build_vocab(train,\n",
    "                     min_freq=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural net structure\n",
    "Input: two sequences\n",
    "[?1 x 1], [?2 x 1]\n",
    "\n",
    "Embedding layer:\n",
    "-> [n_vocab, embedding_dim] -> [?1 x embedding_dim], [?2 x embedding_dim]\n",
    "\n",
    "Concatenation:\n",
    "-> [?1 + ?2 x embedding_dim]\n",
    "\n",
    "GRU:\n",
    "-> [embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 = torch.tensor([1, 2, 0])\n",
    "# x2 = torch.tensor([2, 0])\n",
    "\n",
    "# emb = nn.Embedding(3, 10)\n",
    "\n",
    "# x1_emb = emb(x1)\n",
    "# x2_emb = emb(x2)\n",
    "\n",
    "# assert x1_emb.size() == torch.Size([3, 10]) and x2_emb.size() == torch.Size([2, 10])\n",
    "\n",
    "# concatenated = torch.cat([x1_emb, x2_emb])\n",
    "\n",
    "# assert concatenated.size() == torch.Size([5, 10])\n",
    "\n",
    "# gru = nn.GRU(input_size=10, hidden_size=8)\n",
    "# _, x = gru(concatenated.view(-1, 1, 10))\n",
    "\n",
    "# assert x.size() == torch.Size([1, 1, 8])\n",
    "\n",
    "# linear = nn.Linear(8, 1)\n",
    "\n",
    "# out = torch.sigmoid(linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNNGRUSequential(len(question.vocab), 300, 128).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    train_iter = data.Iterator(train,\n",
    "                               batch_size=256,\n",
    "                               repeat=False,\n",
    "                               shuffle=True)\n",
    "    \n",
    "    val_iter = data.Iterator(val,\n",
    "                             batch_size=64)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, sample in enumerate(train_iter):\n",
    "        x1 = sample.question1.to(device)\n",
    "        x2 = sample.question2.to(device)\n",
    "        target = sample.is_duplicate.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = rnn(x1, x2)\n",
    "        loss = criterion(output, target.view(1, -1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print('Batch: {} Loss: {}'.format(i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for sample in val_iter:\n",
    "            x1 = sample.question1.to(device)\n",
    "            x2 = sample.question2.to(device)\n",
    "            target = sample.is_duplicate.type(torch.ByteTensor).to(device)\n",
    "            output = rnn(x1, x2)\n",
    "            output = output.view(-1)\n",
    "            pred = output >= 0.5\n",
    "            correct += (target == pred).sum().item()\n",
    "            total += 64\n",
    "        \n",
    "    print('Val accuracy: {}'.format(correct / total * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuoraDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, train=False, transform=None):\n",
    "        self.questions = pd.read_csv(csv_file, keep_default_na=False)\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sample = {'q1': self.questions.loc[idx, 'question1'],\n",
    "                  'q2': self.questions.loc[idx, 'question2']}\n",
    "        \n",
    "        if self.train:\n",
    "            sample['target'] = self.questions.loc[idx, 'is_duplicate']\n",
    "            \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def get_tokens(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QuoraDataset('train.csv', train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = defaultdict(int)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    questions = [tokenizer(sample['q1']), tokenizer(sample['q2'])]\n",
    "    for token in chain(*questions):\n",
    "        word = token.text.lower()\n",
    "        if token.is_stop:\n",
    "            continue\n",
    "        else:\n",
    "            word_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_words = sorted([(word, counts) for word, counts in word_dict.items()], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordTokenizer:\n",
    "    \n",
    "    def __init__(self, num_words):\n",
    "        self.num_words = num_words\n",
    "    \n",
    "    def tokenize(self, *args):\n",
    "        words_counts = defaultdict(int)\n",
    "        \n",
    "        for text in chain(*args):\n",
    "            for token in tokenizer(text):\n",
    "                word = token.text.lower()\n",
    "                if token.is_stop:\n",
    "                    continue\n",
    "                else:\n",
    "                    words_counts[word] += 1\n",
    "        \n",
    "        words_counts = sorted([(word, counts) for word, counts in words_counts.items()], key=lambda x: x[1], reverse=True)\n",
    "        self.words_dict = {word: i for i, (word, counts) in enumerate(sorted_words, 1)}\n",
    "    \n",
    "    def __call__(self, seq):\n",
    "        int_seq = [self.words_dict[word] for word in ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{word: i for i, (word, counts) in enumerate(sorted_words, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fun(*args):\n",
    "    for i in chain(*args):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset.questions['question1'].values:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = WordTokenizer(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_fun(dataset.questions['question1'].values, dataset.questions['question2'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer.tokenize(dataset.questions['question1'].values, dataset.questions['question2'].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
